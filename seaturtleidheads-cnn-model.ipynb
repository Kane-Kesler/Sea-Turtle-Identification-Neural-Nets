{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SeaTurtleIDHeads CNN Model\n\n\nThis notebook provides a CNN model using the [SeaTurtleIDHeads dataset](https://www.kaggle.com/datasets/wildlifedatasets/seaturtleidheads). We load the necessary packages first.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom datetime import date\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom utils import load_dataframe, plot_grid\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torchvision.transforms import ToTensor\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\n\nroot_dir = '/kaggle/input/seaturtleidheads'","metadata":{"execution":{"iopub.status.busy":"2023-05-09T10:29:44.435237Z","iopub.execute_input":"2023-05-09T10:29:44.435678Z","iopub.status.idle":"2023-05-09T10:29:44.444222Z","shell.execute_reply.started":"2023-05-09T10:29:44.435636Z","shell.execute_reply":"2023-05-09T10:29:44.442879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters \nnum_epochs = 50\nbatch_size = 15\nlearning_rate = 0.001","metadata":{"execution":{"iopub.status.busy":"2023-05-09T10:29:44.450010Z","iopub.execute_input":"2023-05-09T10:29:44.450334Z","iopub.status.idle":"2023-05-09T10:29:44.455709Z","shell.execute_reply.started":"2023-05-09T10:29:44.450290Z","shell.execute_reply":"2023-05-09T10:29:44.454676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load data and investigate structure\ndf = load_dataframe(root_dir, img_type='heads')\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T10:29:44.458519Z","iopub.execute_input":"2023-05-09T10:29:44.459417Z","iopub.status.idle":"2023-05-09T10:29:44.555301Z","shell.execute_reply.started":"2023-05-09T10:29:44.459378Z","shell.execute_reply":"2023-05-09T10:29:44.554164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a subset of data\nsubset_df = df[df['identity'].isin(['t023', 't025', 't033', 't034',\n                                    't073', 't094', 't204', 't217',\n                                    't243', 't323'])]\n\nsubset_df = subset_df.reset_index(drop=True)\n\n# sample of subset data\nplot_grid(df[(df['identity'] == 't094')], root_dir, n_cols=5, n_rows=1, img_min=120)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T10:29:44.557953Z","iopub.execute_input":"2023-05-09T10:29:44.558392Z","iopub.status.idle":"2023-05-09T10:29:44.764440Z","shell.execute_reply.started":"2023-05-09T10:29:44.558332Z","shell.execute_reply":"2023-05-09T10:29:44.763609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split(data, date_col_name='', split_date = '' , time_cutoff=False):\n    \n    \"\"\"\n    Splits data frame with either time cutoff or random split\n    with a ratio of roughly 60:20:20 of training, validation and testing data.\n\n    Parameters:\n    data (df): dataframe containing datetime information.\n    date_col_name (str): name of column containing datetime information.\n    split_date (str): splits the dataset into training and testing for a given datetime.\n    time_cutoff (bool): determines if the split is time-cutoff or random.\n\n    Returns:\n    tuple:\n        train_df (df)\n        val_df (df)\n        test_df (df)\n\n    Raises:\n    ValueError: If the user declares time_cutoff=True but does not specify date_col_name or split_date\n                an error tells the user to specify them.\n    \"\"\"\n    \n     # Random split\n    if time_cutoff == False:\n        train_val_df, test_df = train_test_split(data, \n                                                 test_size=0.21, random_state=42) \n\n        train_df, val_df = train_test_split(train_val_df,\n                                            test_size=0.20, random_state=42) \n        \n    # Time-cutoff split\n    if time_cutoff == True:\n        if date_col_name != '' and split_date !='':\n            split_date = split_date\n            train_val_df = data.loc[data[date_col_name] <= split_date]\n            train_df, val_df = train_test_split(train_val_df, test_size=0.20, random_state=42) \n            test_df = subset_df.loc[split_date <= subset_df[date_col_name]]\n\n        else:\n            raise ValueError(\"Provide a value for argument date_col_name or split_date\")\n        \n    # reset their indexes\n    train_df.reset_index(drop=True, inplace=True)\n    val_df.reset_index(drop=True, inplace=True)\n    test_df.reset_index(drop=True, inplace=True)\n        \n    return train_df, val_df, test_df\n\ntime_cutoff=False\ntrain_df, val_df, test_df = split(subset_df, date_col_name='date',split_date = '2020-01-01 00:00:00', time_cutoff=time_cutoff)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T10:29:44.765720Z","iopub.execute_input":"2023-05-09T10:29:44.766566Z","iopub.status.idle":"2023-05-09T10:29:44.778338Z","shell.execute_reply.started":"2023-05-09T10:29:44.766534Z","shell.execute_reply":"2023-05-09T10:29:44.777481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# give labels corresponding number\nlabel_arr = subset_df['identity'].unique()\nlabel_to_id = {}\nid_to_label = {}\nindex = 0\n\nfor class_name in label_arr:\n    label_to_id[class_name] = str(index)\n    id_to_label[str(index)] = class_name\n    index += 1\n    \nprint(label_to_id)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T10:29:44.780467Z","iopub.execute_input":"2023-05-09T10:29:44.780845Z","iopub.status.idle":"2023-05-09T10:29:44.792842Z","shell.execute_reply.started":"2023-05-09T10:29:44.780785Z","shell.execute_reply":"2023-05-09T10:29:44.791659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating the dataset\nclass SeaTurtleDataset(Dataset):\n    \n    def __init__(self, dataframe, root_dir, transform=None):\n        self.dataframe = dataframe\n        self.root_dir = root_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.root_dir, self.dataframe.iloc[idx, 4])\n        image1 = cv2.imread(img_name)\n        if image1 is None:\n            raise ValueError(f\"Unable to read image: {img_name}\")\n        image = Image.fromarray(image1)\n        \n        label_key = self.dataframe.iloc[idx, 1]\n        label = torch.tensor(int(label_to_id[label_key]))\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T10:29:44.794479Z","iopub.execute_input":"2023-05-09T10:29:44.794851Z","iopub.status.idle":"2023-05-09T10:29:44.806141Z","shell.execute_reply.started":"2023-05-09T10:29:44.794821Z","shell.execute_reply":"2023-05-09T10:29:44.805222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define transforms\nimg_size= 32\n\ntrain_transform = transforms.Compose(\n    [transforms.RandomHorizontalFlip(p=0.5),\n     transforms.RandomVerticalFlip(p=0.5),\n     transforms.RandomRotation(degrees=10),\n     transforms.ToTensor(),\n     transforms.Resize((img_size, img_size)),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nval_transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Resize((img_size, img_size)),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntest_transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Resize((img_size, img_size)),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])","metadata":{"execution":{"iopub.status.busy":"2023-05-09T10:29:44.807617Z","iopub.execute_input":"2023-05-09T10:29:44.807920Z","iopub.status.idle":"2023-05-09T10:29:44.817985Z","shell.execute_reply.started":"2023-05-09T10:29:44.807895Z","shell.execute_reply":"2023-05-09T10:29:44.816562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create dataset and dataloader for each dataframe\ntrain_dataset = SeaTurtleDataset(train_df, root_dir=root_dir, transform=train_transform)\ntrain_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n                                           num_workers=2, shuffle=True)\n\nval_dataset = SeaTurtleDataset(val_df, root_dir=root_dir, transform=val_transform)\nval_loader  = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, \n                                           num_workers=2, shuffle=False)\n\ntest_dataset = SeaTurtleDataset(test_df, root_dir=root_dir,transform=test_transform)\ntest_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, \n                                           num_workers=2, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T10:29:44.819583Z","iopub.execute_input":"2023-05-09T10:29:44.819868Z","iopub.status.idle":"2023-05-09T10:29:44.828611Z","shell.execute_reply.started":"2023-05-09T10:29:44.819842Z","shell.execute_reply":"2023-05-09T10:29:44.827454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# exmaple of images\nimg_num = 5\n\ntrain_features, train_labels = next(iter(train_loader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"labels batch shape: {train_labels.size()}\")\nfor i in range(img_num):\n    img = train_features[i].squeeze()\n    label = train_labels[i]\n    plt.imshow(img.T)\n    plt.show()\n    print(f\"label: {label}\")\n    print(type(train_loader))","metadata":{"execution":{"iopub.status.busy":"2023-05-09T10:29:44.830064Z","iopub.execute_input":"2023-05-09T10:29:44.830472Z","iopub.status.idle":"2023-05-09T10:29:47.305934Z","shell.execute_reply.started":"2023-05-09T10:29:44.830436Z","shell.execute_reply":"2023-05-09T10:29:47.304605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the network architecture\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        # convolutional layer (sees 32x32x3 image tensor)\n        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n        # convolutional layer (sees 16x16x16 tensor)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        # convolutional layer (sees 8x8x32 tensor)\n        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n        # max pooling layer\n        self.pool = nn.MaxPool2d(2, 2)\n        # linear layer (64 * 4 * 4 -> 500)\n        self.fc1 = nn.Linear(64 * 4 * 4, 500)\n        # linear layer (500 -> 250)\n        self.fc2 = nn.Linear(500, 250)\n        # linear layer (250 -> 10)\n        self.fc3 = nn.Linear(250, 10)\n        # dropout layer (p=0.25)\n        self.dropout = nn.Dropout(0.25)\n        # batch normalization for convolution layer 1 output\n        self.bn_conv1 = nn.BatchNorm2d(16)\n        # batch normalization for convolution layer 2 output\n        self.bn_conv2 = nn.BatchNorm2d(32)\n        # batch normalization for convolution layer 3 output\n        self.bn_conv3 = nn.BatchNorm2d(64)\n        # batch noramlization for fully connected layer 1 output\n        self.bn_fc1 = nn.BatchNorm1d(500) \n        # batch noramlization for fully connected layer 2 output\n        self.bn_fc2 = nn.BatchNorm1d(250) \n        \n\n    def forward(self, x):\n        # add sequence of convolutional and max pooling layers (batch normalization)\n        x = self.pool(F.relu(self.bn_conv1(self.conv1(x))))\n        x = self.pool(F.relu(self.bn_conv2(self.conv2(x))))\n        x = self.pool(F.relu(self.bn_conv3(self.conv3(x))))\n        # flatten image input\n        x = x.view(-1, 64 * 4 * 4)\n        # add dropout layer\n        x = self.dropout(x)\n        # add 1st hidden layer, with elu activation function\n        x = F.relu(self.bn_fc1(self.fc1(x)))\n        # add dropout layer\n        x = self.dropout(x)\n        # add 2nd hidden layer, with elu activation function\n        x = F.relu(self.bn_fc2(self.fc2(x)))\n        # add dropout layer\n        x = self.dropout(x)\n        # add 3rd layer, with elu activation function\n        x = self.fc3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-05-09T10:29:47.308907Z","iopub.execute_input":"2023-05-09T10:29:47.309214Z","iopub.status.idle":"2023-05-09T10:29:47.321606Z","shell.execute_reply.started":"2023-05-09T10:29:47.309186Z","shell.execute_reply":"2023-05-09T10:29:47.320802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ConvNet().to(device)\n\n# define loss and optimizer and learning rate scheduler\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)\n\n# these lists will be used to plot the accuracy and loss against epochs\ntrain_loss_hist, train_acc_hist = [], []\nvalid_loss_hist, valid_acc_hist = [], []\ntrain_total_steps = len(train_dataset)\nvalid_total_steps = len(val_dataset)\n\n# Train the model\nfor epoch in range(num_epochs):\n    # track the training loss and the number of correct predictions from training\n    train_loss = 0\n    train_correct = 0\n    for i, (images, labels) in enumerate(train_loader):\n        # origin shape: [20, 3, 32, 32] = 4, 3, 1024\n        # input_layer: 3 input channels, 16 output channels, 3 kernel size\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # calculate training statistics\n        _, predicted = torch.max(outputs, 1)\n        train_correct += (predicted == labels).sum().item()\n\n        # update the loss\n        train_loss += loss.item()\n\n    train_avg_loss = train_loss / train_total_steps\n    train_avg_acc = train_correct / train_total_steps\n    train_loss_hist.append(train_avg_loss)\n    train_acc_hist.append(train_avg_acc)\n\n\n    # Validate the model\n    # turn off dropout and batch normalization\n    model.eval()\n    with torch.no_grad():\n        valid_loss = 0\n        valid_correct = 0\n        for i, (images, labels) in enumerate(val_loader):\n            images = images.to(device)\n            labels = labels.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            # calculate training statistics\n            _, predicted = torch.max(outputs, 1)\n            valid_correct += (predicted == labels).sum().item()\n\n            # update the loss\n            valid_loss += loss.item()\n\n        valid_avg_loss = valid_loss / valid_total_steps\n        valid_avg_acc = valid_correct / valid_total_steps\n        valid_loss_hist.append(valid_avg_loss)\n        valid_acc_hist.append(valid_avg_acc)\n\n    # turn dropout and bacth normalization back on\n    model.train()\n\n    # update learning rate\n    scheduler.step(valid_avg_loss)\n    print(f'[epoch] {epoch+1} train loss: {train_avg_loss:.5f} train acc: {train_avg_acc:.4f} valid loss: {valid_avg_loss:.5f} valid acc: {valid_avg_acc:.4f}')\n\n\nprint('Finished Training')\n\n# Plot training statistics\nif time_cutoff==False:\n    plt.title('Model Loss (Random Split)')\nelse:    \n    plt.title('Model Loss (Time-cutoff Split)')\n\nlegend = ['Train', 'Validation']\nplt.plot(train_loss_hist)\nplt.plot(valid_loss_hist)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(legend, loc='upper right')\nplt.show()\n\nlegend = ['Train', 'Validation']\nplt.plot(train_acc_hist)\nplt.plot(valid_acc_hist)\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(legend, loc='upper right')\nplt.show()\n\n# Save the trained model\ntorch.save(model.state_dict(),'checkpoint.pth')\n\n# Download checkpoint file\nmodel.load_state_dict(torch.load('checkpoint.pth'))\n\n# testing the model\nmodel.eval()\nwith torch.no_grad():\n    nb_classes = 10\n    confusion_matrix = np.zeros((nb_classes, nb_classes))\n    label_list=[]\n    prediction_list=[]\n    for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            for file in labels:\n                label_list.append(file)\n            for pred in preds:\n                prediction_list.append(id_to_label[str(pred.item())])\n            for j, k in zip(labels.view(-1), preds.view(-1)):\n                confusion_matrix[j.long(), k.long()] += 1\n\n# produce confusion matrix\nplt.figure(figsize=(15,10))\n\nunique_class_names = list(id_to_label.values())\ndf_cm = pd.DataFrame(confusion_matrix, index=unique_class_names, columns=unique_class_names).astype(int)\nheatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=15)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=15)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n;                    \n\n\npred_df = pd.DataFrame({'image_id': label_list, \"label\": prediction_list})\ntest_acc = 100*(pred_df['label'] == test_df['identity']).sum()/len(pred_df)\nprint(f'test accuracy: {test_acc}')\n\nconfusion_matrix","metadata":{"execution":{"iopub.status.busy":"2023-05-09T10:29:47.322782Z","iopub.execute_input":"2023-05-09T10:29:47.323245Z","iopub.status.idle":"2023-05-09T10:40:04.863043Z","shell.execute_reply.started":"2023-05-09T10:29:47.323216Z","shell.execute_reply":"2023-05-09T10:40:04.861796Z"},"trusted":true},"execution_count":null,"outputs":[]}]}